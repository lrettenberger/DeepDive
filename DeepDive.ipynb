{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before anything else\n",
    "\n",
    "Please fill out the survey at the following link for the evaluation of the lecture:\n",
    "[Onlineumfrage Z8FV5](https://onlineumfrage.kit.edu/evasys/online.php?p=Z8FV5)\n",
    "\n",
    "\n",
    "![image info](./graphics/QRCode_Computational%20Intelligence.png)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Classroom\n",
    "I am happy to have you. In the following, you will interactively experience and explore the world of artificial intelligence, machine learning, or, even more specifically, deep learning. You will get to know...\n",
    "- the general design of a deep learning pipeline\n",
    "- the basic principles behind deep learning \n",
    "- the fundamental neural network architecture\n",
    "- the process of training a neural network\n",
    "- the deployment of your trained neural network\n",
    "- the validation of your model\n",
    "\n",
    "After this course, you will know the basics of...\n",
    "- the look and feel of the programming language python\n",
    "- the deep learning packages tensorflow and Keras\n",
    "- the intuitions and approaches within deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Getting comfortable in the driver's seat!\n",
    "\n",
    "The interface you are seeing in front of you is called jupyter notebook, an excellent interactive way to present and communicate code. Think of it as your cockpit. The nice thing is that in the back, we already got your engine preheated and ready to rumble, waiting for you to kick the pedal to the metal. Thanks to [binder](https://mybinder.org/) you can execute the code, on your machine using binders compute power.  \n",
    "\n",
    "In principle, this notebook is structured in cells, which get highlighted once you select them. There are two types of cells. The markdown cells (such as this one) are for everything that is not code. Here you will find explanations, descriptions, figures, and many more. The other type of cell is for code. Here you can interactively change code and run it within your browser. \n",
    "\n",
    "Navigate to the next cell by using the down-arrow or click on the next cell...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Great job. This is an interactive python cell with an easy print function. Feel free to change and play around with it.\n",
    "\n",
    "print('Ready Player One!')\n",
    "\n",
    "# A hashtag is used to comment on stuff that is not executed as code. This is useful if you want to add an explanation or further relevant information to your code.\n",
    "# Run a cell by clicking the Run button above, or by Ctrl + Enter (Strg + Enter) \n",
    "\n",
    "# Notice the output below the cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# We will use this cell to import all the packages you will need in the following - think of it as turning on all your systems\n",
    "# in your cockpit\n",
    "\n",
    "# This makes sure that if you change code in your external scripts, they will be updated\n",
    "\n",
    "import checker\n",
    "import generator\n",
    "import training_module\n",
    "import os\n",
    "\n",
    "import math\n",
    "import torchmetrics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "from torchsummary import summary\n",
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "\n",
    "import importlib\n",
    "importlib.reload(checker)\n",
    "importlib.reload(generator)\n",
    "importlib.reload(training_module)\n",
    "\n",
    "# now go ahead and Run the cell. This might take a while...\n",
    "# while the cell is running, you will see ln[*] next to it. Once it finished, you will see the number of execution\n",
    "# In case you want to interrupt the run of a cell, press Ctrl + C (Strg + C) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hint: \n",
    "Below code cells you sometimes find, what we call hints. So in case you get stuck, or are in search of additional information this is the place for additional elaboration on the topic or links. <br>\n",
    "- [print()](https://docs.python.org/3/library/functions.html#print) <br>\n",
    "- [import](https://docs.python.org/3/reference/simple_stmts.html#import)\n",
    "\n",
    "##### Side Quest:\n",
    "Another feature is the side quests. These are things you could try, think about, and tease your brain with. They are not necessary for you to learn the fundamentals. They are nevertheless, valuable for your holistic understanding of the subject. Also, they are fun."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Making sure you are still on track\n",
    "\n",
    "Next, welcome to the world of what we call sanity checks.\n",
    "Every now and then, you will find sanity checks below your interactive code cells. \n",
    "These are to make sure that your code does what it should (this is usually called a [unit test](https://en.wikipedia.org/wiki/Unit_testing)).\n",
    "Once your code passed the unit test, it is safe for you to go ahead with the notebook.\n",
    "So do not touch the code within a sanity check cell. Instead, just run it and fix your code in the respective cell.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Write a function that adds a to b and returns c\n",
    "# the code goes here comment has to be removed and indicates that you need to pick your brain and write some code.\n",
    "def add(a,b):\n",
    "    c = '''your code goes here'''\n",
    "    return c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### Hint: \n",
    "An addition in python can be done by simply writing a '+' sign connecting the two input arguments. An alternative is to use a dedicated function that takes an array of values. <br>\n",
    "- [sum([a, b])](https://docs.python.org/3/library/functions.html#sum) <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# After writing the add function in the cell above, go ahead and run this cell.\n",
    "\n",
    "'''\n",
    " ____   __   __ _  __  ____  _  _     ___  _  _  ____  ___  __ _ \n",
    "/ ___) / _\\ (  ( \\(  )(_  _)( \\/ )   / __)/ )( \\(  __)/ __)(  / )\n",
    "\\___ \\/    \\/    / )(   )(   )  /   ( (__ ) __ ( ) _)( (__  )  ( \n",
    "(____/\\_/\\_/\\_)__)(__) (__) (__/     \\___)\\_)(_/(____)\\___)(__\\_)\n",
    "\n",
    "Do not touch, just run!\n",
    "'''\n",
    "checker.test_add(add)\n",
    "\n",
    "\n",
    "# If your code is working correctly, you will be prompted with the message \"Everything passed, \n",
    "# you are ready to go.\"\n",
    "# This means you can continue with the next cell\n",
    "\n",
    "# If there is a bug in your code, you will be prompted with an AssertionError, \n",
    "# outlining how your code failed.\n",
    "\n",
    "# Go ahead, break your code and see what's happening to get a feeling for the sanity checks. \n",
    "# Then feel free to move on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you know how to drive this thing, you are ready for some real action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Talking about brains\n",
    "\n",
    "Your brain is the most astonishing multi-purpose computer around. And neuroscientists and engineers alike have been trying to understand the mechanics of brains, to apply their findings to other fields, such as computer science. This is how the idea of artificial neural networks came into being. <br>\n",
    "\n",
    "Your brain comprises approximately 80 billion neurons, or brain cells, which are heavily interconnected with what are called synapses. Neurons communicate with each other by sending electrical impulses through these synapses. If a neuron receives enough of these messages, the neuron itself sends an impulse to the neurons it is connected with. In this way, the brain can process complex data, extract features, and recognize a pattern. <br>\n",
    "\n",
    "And this is not yet, the most exciting ability of the brain. Your brain is further able to learn, adjusting to newly perceived data, patterns, and behavior. This happens by either strengthening or weakening the connection or synapses between neurons, influencing their interplay. <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, we will build something that is very similar to a very very simple version of a brain.\n",
    "\n",
    "For this we will use:\n",
    "- .. [PyTorch](https://pytorch.org/) \n",
    "- .. [Lightning](https://lightning.ai/)\n",
    "\n",
    "while clambering on the \"Hello World!\" of deep learning, the [MNIST](https://en.wikipedia.org/wiki/MNIST_database) dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This line imports PyTorch\n",
    "import torch    \n",
    "# And this one Lightning\n",
    "import pytorch_lightning as pl\n",
    "# There is this joke that once you have added this import line to your code, \n",
    "# you are allowed to sell your product telling everyone that you are using AI. Life is easy, sometimes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the MNIST dataset\n",
    "\n",
    "Before any data science or machine learning project, it is essential to get to know your data. This will enable you to detect issues, noise, pitfalls and understand what your model will learn at the end of the day. With the MNIST dataset, you will be working with a beautiful, cleaned, easy to understand, low-memory, large-scale dataset. \n",
    "Full disclaimer, MNIST is excellent for learning and research purposes, yet this is not what you can expect in real-life. \n",
    "\n",
    "MNIST is so commonly used that the PyTorch packages got our back with loading the data in one line. We are loading pairs of samples and ground truth annotations for both the training set (for training our model) and the test set (for testing our model). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.datasets as datasets\n",
    "# And thats how easy we import the most popular deep learning datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=None)\n",
    "mnist_testset = datasets.MNIST(root='./data', train=False, download=True, transform=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets see what this variable contains.\n",
    "\n",
    "print(mnist_trainset)\n",
    "print('--------------------------------------------------------------------')\n",
    "print(mnist_trainset[0])\n",
    "print('--------------------------------------------------------------------')\n",
    "print(mnist_testset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we have $60k$ datapoints in the training split, so $60k$ images, and that we can index the variable (``[0]``). The first entry of the training split contains an image of size $28x28$ (``PIL.Image.Image image mode=L size=28x28``) and the integer ``5``. The test split contains $10k$ images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets extract the actual datapoints and annotations so that we can explore them\n",
    "x_train = mnist_trainset.data.numpy()\n",
    "y_train = mnist_trainset.targets.numpy()\n",
    "x_test = mnist_testset.data.numpy()\n",
    "y_test = mnist_testset.targets.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n Plot of the first 25 samples in the MNIST training set')\n",
    "numbers_to_display = 25\n",
    "num_cells = math.ceil(math.sqrt(numbers_to_display))\n",
    "plt.figure(figsize=(10,10))\n",
    "for i in range(numbers_to_display):\n",
    "    plt.subplot(num_cells, num_cells, i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(x_train[i], cmap=plt.cm.binary)\n",
    "    plt.xlabel(y_train[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape of the data arrays already gave away that we are using way more samples (60k) for training than for testing (10k). A typical ratio is 80% training and 20% testing as a rule of thumb. Actually, the 80% training is further split into the real training and a validation set, but that will not bother us here. \n",
    "\n",
    "As we plot the first 25 samples of the MNIST training set together with their ground truth labels (annotations) we can start to guess what the dataset is all about - it is a dataset purposed for digit recognition.\n",
    "\n",
    "\n",
    "### Data preprocessing\n",
    "We already talked quite a bit about [activation functions](https://cs231n.github.io/neural-networks-1/) (or non-linearity) and how they take a single, usually accumulated, number and perform a specific fixed mathematical operation on it. There are quite different ones, but you will soon find out that there emerges a shared pattern within deep neural networks that makes them beneficial in practice.\n",
    "\n",
    "- They mostly map any incoming value so that they are asymptotically bounded from above and below. For example, the [sigmoid function](https://en.wikipedia.org/wiki/Sigmoid_function) does map to a range of values between $0$ and $1$.\n",
    "They are usually most sensitive around zero because they tend to saturate to the bounds of the mapping for large numbers.\n",
    "\n",
    "Thus we can speed up the training process by preprocessing our data so that the first layer's activation function has an easier time picking up the pattern.\n",
    "\n",
    "For the MNIST, this is quickly done by normalizing the grayscale values $[0, 255]$ to a range between $[0, 1]$. Remember to do the preprocessing for both the training and the test samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_normalized = x_train  #'''your code goes here'''\n",
    "x_test_normalized = x_test #'''your code goes here'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### Hint: \n",
    "In python a division can be done like this in python:  $3, 5~/~ 2 == 2.5$ <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    " ____   __   __ _  __  ____  _  _     ___  _  _  ____  ___  __ _ \n",
    "/ ___) / _\\ (  ( \\(  )(_  _)( \\/ )   / __)/ )( \\(  __)/ __)(  / )\n",
    "\\___ \\/    \\/    / )(   )(   )  /   ( (__ ) __ ( ) _)( (__  )  ( \n",
    "(____/\\_/\\_/\\_)__)(__) (__) (__/     \\___)\\_)(_/(____)\\___)(__\\_)\n",
    "\n",
    "Do not touch, just run!\n",
    "'''\n",
    "checker.test_normalize(x_train, x_train_normalized)\n",
    "checker.test_normalize(x_test, x_test_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in the next step, we also need to reshape our input to fit our input layer later on. \n",
    "# This is due to pytorch expecting a definition for how many channels your input sample has, as we \n",
    "# deal with gray scale this is 1.\n",
    "x_train_normalized= x_train_normalized.reshape(-1, 28, 28, 1)\n",
    "x_test_normalized = x_test_normalized.reshape(-1, 28, 28, 1)\n",
    "\n",
    "# To work with PyTorch we also need to convert our numpy arrays to tensors.\n",
    "x_train_normalized = torch.from_numpy(x_train_normalized).float()\n",
    "x_test_normalized = torch.from_numpy(x_test_normalized).float()\n",
    "y_train = torch.from_numpy(y_train)\n",
    "y_test = torch.from_numpy(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Designing the neural network architecture\n",
    "\n",
    "Remember how much work it was to set up a single neuron - right. Now enjoy how PyTorch is helping you with this. We will call our neural network [marvin](https://en.wikipedia.org/wiki/Marvin_the_Paranoid_Android). Feel free to change it to whatever you like, in case you are not okay with it.\n",
    "\n",
    "Our model will follow some design requirements, it will be [sequential](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html) or it other words, feed forward, meaning that once a value passed through a unit, the unit will not see a value during that forward pass again. Or in other words, we do not have any recurrence in the architecture (feedback loops and so on). \n",
    "\n",
    "That's a lot to tackle, there will be quite some hyperparameters. This is for the visual learner - need a sandbox before we get started? I got your back [tensorflow sandbox](http://playground.tensorflow.org).\n",
    "\n",
    "I really encourage you to take the time to play in the sandbox. It's worth to build up some intuition. At least play with the number of layers and the number of units per layer. Observe how the features increase in complexity the deeper you go with your layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the moment where you define your model's architecture\n",
    "marvin = torch.nn.Sequential(\n",
    "  torch.nn.Flatten(),\n",
    "  torch.nn.Linear(in_features=784,out_features=10),\n",
    "  '''your code goes here'''\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding layers to Marvin is as easy as adding a layer while handing over that layer's specification. In the end, the layers will be stacked following the order in which your code gets compiled.\n",
    "\n",
    "Initially, we need an input layer, which can take in our input samples. We want to build a Neural Network which consists of fully connected layers. Such layers can't cope with three-dimensional image inputs, they want a single, one-dimensional vector! So we need to flatten our images. This is done with the [Flatten](https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html) layer. Once we have that set-up, we feed our images through one [Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear) (or fully-connected) layer.\n",
    "Now, print out your model's summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(marvin,(1,28,28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### Hint: \n",
    "Find out about activation functions [here](https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity) and about layers [here](https://pytorch.org/docs/stable/nn.html#linear-layers). For each Linear layer you need to define how many input features you want, and how many output features should be produced. For the first Linear layer the input features are the pixel values of the flatten images, so $28x28=784$. \n",
    "\n",
    "<br>    \n",
    "\n",
    "But wait! There is something missing. Keep on designing your model until your model summary is the same as this one:\n",
    "\n",
    "```\n",
    "----------------------------------------------------------------\n",
    "        Layer (type)               Output Shape         Param #\n",
    "================================================================\n",
    "           Flatten-1                  [-1, 784]               0\n",
    "            Linear-2                   [-1, 10]           7,850\n",
    "           Softmax-3                   [-1, 10]               0\n",
    "================================================================\n",
    "Total params: 7,850\n",
    "Trainable params: 7,850\n",
    "Non-trainable params: 0\n",
    "----------------------------------------------------------------\n",
    "Input size (MB): 0.00\n",
    "Forward/backward pass size (MB): 0.01\n",
    "Params size (MB): 0.03\n",
    "Estimated Total Size (MB): 0.04\n",
    "----------------------------------------------------------------\n",
    "```\n",
    "\n",
    "Try thinking about _why_ you chose which layer to create your model? What are we trying to predict? Using softmax in your output layer will give you the pleasant effect of a probability distribution for your prediction. The output of softmax transformation has all values non-negative and sum to 1.\n",
    "\n",
    "##### Side Quest: \n",
    "Come back once you finished training your model and try adding layers to get a better score. Maybe add another Linear layer before the Softmax? But keep in mind that you should add a activation function ([ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU)?) after a intermediate layer. Adding [Dropout](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html#torch.nn.Dropout) layer to get some fancy regularization will most certainly also help. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    " ____   __   __ _  __  ____  _  _     ___  _  _  ____  ___  __ _ \n",
    "/ ___) / _\\ (  ( \\(  )(_  _)( \\/ )   / __)/ )( \\(  __)/ __)(  / )\n",
    "\\___ \\/    \\/    / )(   )(   )  /   ( (__ ) __ ( ) _)( (__  )  ( \n",
    "(____/\\_/\\_/\\_)__)(__) (__) (__/     \\___)\\_)(_/(____)\\___)(__\\_)\n",
    "\n",
    "Do not touch, just run!\n",
    "'''\n",
    "checker.test_neural_network(marvin)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Designing the neural network training\n",
    "\n",
    "Great, that's it. Take a moment to realize how quickly and with how much ease (maybe not during the first time) you designed this architecture.\n",
    "\n",
    "Next, we need to train Marvin to assign a class prediction when we hand him over a sample of a digit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define your loss\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "print(-math.log(1/10))\n",
    "\n",
    "sampleID = 100\n",
    "loss_fn(marvin(x_train_normalized[sampleID-1:sampleID]),y_train[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss is crucial for your training. The better your loss reflects the actual objective of your model the more efficient it will converge to the optimal parameter set. \n",
    "\n",
    "The loss is also great to check whether your model behaves as expected. The SparseCategoricalCrossentropy is equal to the negative log probability of the true class: The loss is zero if the model is sure of the correct class.\n",
    "\n",
    "Your untrained model gives probabilities close to random (1/10 for each class,  as there are 10 digits), so the initial loss should be close to:\n",
    "\n",
    "-tf.math.log(1/10) ~= 2.3\n",
    "\n",
    "Obviously different samples will give different losses, play with the sampleID to observe the changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We only use the first 100 samples of the train dataset ([:1000] and the first 500 of the test dataset ([:500]) to accelerate training since binder is quite slow.\n",
    "# Maybe using more samples can increase your performance?\n",
    "train_dataset = TensorDataset(x_train_normalized[:1000], y_train[:1000])\n",
    "train_dl = DataLoader(train_dataset, batch_size=32,shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(x_test_normalized[:500], y_test[:500])\n",
    "test_dl = DataLoader(test_dataset, batch_size=32,shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets define the metric to use. In our case we will use the accuracy, which is the fraction of predictions our model got right.\n",
    "##### Hint: \n",
    "Make sure you understand the difference between a metric (only tells you how good your neural network performs) and a loss (tells you how your model performance, but also directly tells your model how to update its weights to improve performance). <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_pred,y_true):\n",
    "    y_pred_argmax = torch.argmax(y_pred,dim=1)\n",
    "    accuracy = torch.sum(y_pred_argmax =='''your code goes here''')\n",
    "    accuracy = accuracy /len(y_true)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    " ____   __   __ _  __  ____  _  _     ___  _  _  ____  ___  __ _ \n",
    "/ ___) / _\\ (  ( \\(  )(_  _)( \\/ )   / __)/ )( \\(  __)/ __)(  / )\n",
    "\\___ \\/    \\/    / )(   )(   )  /   ( (__ ) __ ( ) _)( (__  )  ( \n",
    "(____/\\_/\\_/\\_)__)(__) (__) (__/     \\___)\\_)(_/(____)\\___)(__\\_)\n",
    "\n",
    "Do not touch, just run!\n",
    "'''\n",
    "checker.test_accuracy(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we define our training module in which we tell PyTorch which model, loss function, and metric to use..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_mod = training_module.TrainingModule(\n",
    "    model=marvin,\n",
    "    loss=loss_fn,\n",
    "    metric=accuracy\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "\n",
    "Now it is time to plug our data into Marvin and let him learn to recognize the digit within an image sample. Besides plugging in the data, and the labels, we need to define how long we want to train Marvin. In deep learning, training on each sample of the training set precisely ones is called an epoch.\n",
    "Obviously, the longer you train, the better your model will fit the underlying pattern. Then you can also set how many samples Marvin will see during each update step (this is called batch size). The more samples, the better the gradient approximation, but at some point, you will also run into hardware limitations.\n",
    "\n",
    "Yet, remember that it is the unseen data of your validation set that machine learning approaches are after. This is also what lets deep learning stand out from pure optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With this variable, we save the progress of our model. Just keep it as it is.\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=os.getcwd(),\n",
    "    filename='marvin',\n",
    "    save_top_k=1,\n",
    "    verbose=True,\n",
    "    monitor='validation_metric',\n",
    "    mode='max',\n",
    "    enable_version_counter=False\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=5, # Maybe training longer will yield better results?\n",
    "    callbacks=checkpoint_callback\n",
    ")\n",
    "\n",
    "# And with that we start the training. Its that simple!\n",
    "trainer.fit(\n",
    "    model=training_mod, \n",
    "    train_dataloaders=train_dl,\n",
    "    val_dataloaders=test_dl\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Side Quest:\n",
    "Tweak your model to get a better performance. To achieve this, you are free to change whatever you want within the architecture and training process. Obviously, better performance is an excellent achievement. You can always submit your current best score to the leaderboard by executing the cell below. Specify your name and a password and your score will pop up on: [the leaderboard!](http://194.164.52.117/). Who will get the best score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import checker\n",
    "importlib.reload(checker)\n",
    "\n",
    "checker.submit_score(\n",
    "    username= # Specify your username\n",
    "    password= # And a password. Dont use a sensitive one that use use elsewhere. Your password is not encrypted!\n",
    "    model=marvin,\n",
    "    checkpoint_path=\"marvin.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Evaluating Marvin\n",
    "\n",
    "Yes, you are right. The job is basically done. Would you say that you know what Marvin learned or what exactly he is capable of in the end? I do not want to spoil it for you, but in real life the hard task begins right after training a model - propper validation is key. Especially in safety critical systems. And there are enough [incidents](https://incidentdatabase.ai/) to demonstrate how models can fail and why it is so hard to prevent these failures.\n",
    "\n",
    "#### Side Quest:\n",
    "Try to answer the following questions:\n",
    "\n",
    "- Which digit is hardest for Marvin to predict correctly?\n",
    "- Which two digits does Marvin confuse most often?\n",
    "- Which digit does he perform best in?\n",
    "- How do digits look that Marvin was not able to predict correctly? Can you see why?\n",
    "\n",
    "These questions are hard to answer and clarify that an in depth analysis of the model's performance is paramount. I like a mix of visualization (which helps my intuition) and number crushing (which helps to write reports and prove my intuition with hard and cold numbers).\n",
    "\n",
    "So in the following, we will go ahead and answer some of the questions mentioned above. Along the way, you will learn some strategies to evaluate your neural network.\n",
    "\n",
    "The worse your model is the easier it will be to pick up effects in your validation, so feel free to either save a random model or a model that has only been trained for a few iterations and use this model in the following."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Intuitive Approach\n",
    "\n",
    "We will start with the intuition building approach. The following code will plot the image samples. The caption below is Marvin's prediction. If he is right, the sample will get a greenish touch. If he is wrong, a reddish one. Try to answer the above questions with the results from the intuitive plot you will generate below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load a saved marvin configuration you want to evaluate\n",
    "# model_name = # '''your model name goes here''' \n",
    "# marvin_reloaded = tf.keras.models.load_model(model_name)\n",
    "\n",
    "checkpoint = \"marvin.ckpt\"\n",
    "trained_model = training_module.TrainingModule.load_from_checkpoint(\n",
    "    checkpoint,\n",
    "    model=marvin,\n",
    "    loss=loss_fn,\n",
    "    metric=accuracy\n",
    ")\n",
    "# Put our model into evaluation mode\n",
    "trained_model.eval()\n",
    "model = trained_model.model\n",
    "model.cpu()\n",
    "\n",
    "# Let Marvin predict on the test set, so we have some data to evaluate his performance.\n",
    "predictions = model(x_test_normalized)\n",
    "\n",
    "# Remember that the prediction of Marvin is a probability distribution over all ten-digit classes\n",
    "# We want him to assign the digit class with the highest probability to the sample.\n",
    "predictions = torch.argmax(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot for the intuitive approach\n",
    "numbers_to_display = 40\n",
    "num_cells = math.ceil(math.sqrt(numbers_to_display))\n",
    "plt.figure(figsize=(15, 15))\n",
    "for plot_index in range(numbers_to_display):    \n",
    "    predicted_label = predictions[plot_index]\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    color_map = 'Greens' if predicted_label == y_test[plot_index] else 'Reds'\n",
    "    plt.subplot(num_cells, num_cells, plot_index + 1)\n",
    "    plt.imshow(x_test_normalized[plot_index].reshape((28, 28)), cmap=color_map)\n",
    "    plt.xlabel(int(predicted_label))\n",
    "\n",
    "plt.subplots_adjust(hspace=1, wspace=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, but now only look at the wrong predictions since they are the ones we are really interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only display wrong predictions\n",
    "numbers_to_display = 40\n",
    "num_cells = math.ceil(math.sqrt(numbers_to_display))\n",
    "plt.figure(figsize=(15, 15))\n",
    "num_of_plotted_numbers = 0\n",
    "for plot_index in range(len(predictions)):    \n",
    "    predicted_label = predictions[plot_index]\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    # what condition needs to be true to only show the number if the prediction is wrong?\n",
    "    if '''your code goes here'''\n",
    "        plt.subplot(num_cells, num_cells, num_of_plotted_numbers + 1)\n",
    "        plt.imshow(x_test_normalized[plot_index].reshape((28, 28)), cmap='Reds')\n",
    "        plt.xlabel(int(predicted_label))\n",
    "        num_of_plotted_numbers+=1\n",
    "    if num_of_plotted_numbers >= numbers_to_display:\n",
    "        break\n",
    "\n",
    "plt.subplots_adjust(hspace=1, wspace=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### The Empirical Number Crushing Approach\n",
    "\n",
    "Now that you intuitively answered the questions, let's do some data analysis on the results and see how much you were off and where you were right and picked up patterns in Marvin's capabilities. \n",
    "\n",
    "#### Hint: \n",
    "Usually, visualizations to building your intuition are most beneficial during your first training runs on problems you do not know what to expect from your neural network. In visualizations, you might also be able to pick up wrong label assignments (think somebody labeled all \"8\"s as \"9\"s), ambiguous or overlapping classes (think somebody labeled all \"6\"s and \"9\"s as the same class). These things do not usually happen in beautifully cleaned datasets such as MNIST. But I promise you, you will be surprised how messy these public datasets out there are. And further, I promise you that if you are building your own datasets, these bugs will be there too and want to be found.\n",
    "\n",
    "Now for the analytical approach, we will look at one of the most basic strategies, the [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix) or error matrix. This table shows the actual class of the sample (horizontal), plotted over which class it was predicted (vertical). To make things clearer, numbers in the diagonal are true positives, meaning Marvin predicted them correctly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = torchmetrics.ConfusionMatrix(task=\"multiclass\", num_classes=10)\n",
    "conf_mat = confusion_matrix(predictions,y_test)\n",
    "\n",
    "f, ax = plt.subplots(figsize=(9, 7))\n",
    "sn.heatmap(\n",
    "    conf_mat,\n",
    "    annot=True,\n",
    "    linewidths=.7,\n",
    "    fmt=\"d\",\n",
    "    square=True,\n",
    "    ax=ax,\n",
    "    cmap=\"viridis\",\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time to say Goodbye\n",
    "Well, actually not quite yet. Now, you are capable of training neural networks, you learned about the fundamentals, and you also can evaluate your model's performance professionally. Next, I want to point out some things you can look into along your path to become a machine learning engineer or deep learning researcher.\n",
    "\n",
    "#### In this notebook\n",
    "You can definitely come back to this notebook again and again. Play with the hyperparameters, try to follow through the notebook with other datasets. There are quite a few [directly supported](https://pytorch.org/vision/stable/datasets.html) by PyTorch, so they can easily plug into this notebook. Or try to load a dataset from your directory and write your own data pipeline.\n",
    "\n",
    "Come back and try working with other layers, such as Dropout or Batchnorm. Get to know regularization strategies, such as augmentation, and try to implement them here as well. Think of this notebook as your bridgehead to the world of deep learning. This is the place to break things and try stuff.\n",
    "\n",
    "#### Literature\n",
    "Reading helps to get to know new approaches, different ideas, and dive deeper and build up your knowledge. I recommend the following books to start with. From there, start to go with the actual papers.\n",
    "\n",
    "[To start with](http://www.deeplearningbook.org/) Deep Learning Book - Ian Goodfellow and Yoshua Bengio and Aaron Courville\n",
    "Abstract: The Deep Learning textbook is a resource intended to help students and practitioners enter the field of machine learning in general and deep learning in particular. The online version of the book is now complete and will remain available online for free.\n",
    "\n",
    "[Coffee Table Solutions](https://www.amazon.de/dp/B09QRGWWZP) The beginner handbook to Applied Deep Learning by my highly valued colleague Mark Schutera.\n",
    "\n",
    "[To understand the fundamentals](http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop%20-%20Pattern%20Recognition%20And%20Machine%20Learning%20-%20Springer%20%202006.pdf) Pattern Recognition and Machine Learning - Bishop Abstract: This new textbook reﬂects these recent developments while providing a comprehensive introduction to the ﬁelds of pattern recognition and machine learning. It is aimed at advanced undergraduates or ﬁrst year Ph.D. students and researchers and practitioners and assumes no previous knowledge of pattern recognition or machine learning concepts.\n",
    "\n",
    "[From theory to practice and back](https://www.ksp.kit.edu/site/books/m/10.5445/KSP/1000008476/)\n",
    "Data Mining in der Medizin und Medizintechnik - Ralf Mikut Zusammenfassung: Dieses Buch systematisiert Ziele, Einsatzszenarien, Vorgehensweisen, Methoden und Anwendungsfelder für eine automatisierte Datenanalyse in der Medizin und Medizintechnik. Es wendet sich hauptsächlich an Doktoranden, Diplom- und Masterstudenten der Ingenieurwissenschaften und Informatik. Im Mittelpunkt steht dabei das Spannungsfeld zwischen medizinischen Anwendern und ihren Zielstellungen sowie den Potenzialen vorhandener Data-Mining-Verfahren.\n",
    "\n",
    "[To get practice](https://www.coursera.org/courses?query=andrew%20ng) Deep Learning and Machine Learning Courses on Coursera - Andrew Ng Abstract: In these courses, you will learn the foundations of Deep Learning, understand how to build neural networks, and learn how to lead successful machine learning projects. You will learn about Convolutional networks, RNNs, LSTM, Adam, Dropout, BatchNorm, Xavier/He initialization, and more. You will work on case studies from healthcare, autonomous driving, sign language reading, music generation, and natural language processing. You will master not only the theory but also see how it is applied in industry. You will practice all these ideas in Python and in TensorFlow, which we will teach.\n",
    "Link: \n",
    "\n",
    "#### Someone deleted the internet, or a link broke in the notebook? \n",
    "You wanted to know more, click a link - and it was dead? That happens, as the internet and especially software is a living thing. In that case, I would be delighted if you'd let me know so that stuff gets fixed for the ones after you. You noticed a bug, a typo, an error, something? Send me an e-mail to Luca.Rettenberger@kit.edu subject: \"AppliedDeepLearningSchool Bug Hunt\".\n",
    "Any other feedback is also highly appreciated.\n",
    "\n",
    "#### Can you use this code for a school or university project?\n",
    "Sure thing, go ahead. Just make sure to cite it appropriately. If you are unsure on how to do this, again, feel free to reach out. I can not wait to see you burning through school and university competitions with deep learning applications. \n",
    "\n",
    "\n",
    "#### Seeking an internship, thesis opportunity, or freelance work in the field of deep learning?\n",
    "Feel free to connect and reach out via e-mail (<Luca.Rettenberger@kit.edu>)  or [LinkedIn](https://www.linkedin.com/in/luca-rettenberger/). \n",
    "\n",
    "<img src=\"graphics/luca_iai_seite.png\" width=\"1000\"><br>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
